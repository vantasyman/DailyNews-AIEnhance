import os
import httpx
from datetime import datetime, timedelta
from tqdm import tqdm
from typing import List, Dict, Any

# å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„æ•°æ®åº“å®¢æˆ·ç«¯
# .db ä¼šè‡ªåŠ¨æ‰¾åˆ°åŒç›®å½•ä¸‹çš„ db.py
from .db import get_db_client

# -----------------------------------------------------------------
# å¸¸é‡å®šä¹‰ (Constants)
# -----------------------------------------------------------------
NEWS_API_BASE_URL = "https://gnews.io/api/v4/search"
# ä¸ºé¿å… API æ»¥ç”¨å’Œæ§åˆ¶ AI æˆæœ¬ï¼Œæˆ‘ä»¬åªå–æ¯ä¸ªä¸»é¢˜æœ€æ–°çš„ 20 ç¯‡æ–‡ç« 
ARTICLES_PER_TOPIC = 30


def fetch_topics_from_db() -> List[Dict[str, Any]]:
    """
    ä»æ•°æ®åº“ 'tracked_topics' è¡¨ä¸­è·å–æ‰€æœ‰æ¿€æ´»çš„å…³é”®è¯ã€‚
    [å¯¹åº” schema.sql è¡¨ 1]
    """
    print("  (Crawler Step 1/3) æ­£åœ¨ä»æ•°æ®åº“è·å–è¿½è¸ªä¸»é¢˜...")
    db = get_db_client()
    try:
        response = db.table("tracked_topics").select("*").eq("is_active", True).execute()
        topics = response.data
        tqdm.write(f"  > æˆåŠŸè·å– {len(topics)} ä¸ªæ¿€æ´»çš„ä¸»é¢˜ã€‚")
        return topics
    except Exception as e:
        tqdm.write(f"ğŸ”´ é”™è¯¯: æ— æ³•ä» 'tracked_topics' è¡¨è·å–æ•°æ®: {e}")
        return []

def fetch_articles_from_api(topic: Dict[str, Any], api_key: str) -> List[Dict[str, Any]]:
    """
    æ ¹æ®å•ä¸ªä¸»é¢˜ï¼Œè°ƒç”¨ NewsAPI è·å–æ–‡ç« ã€‚
    """
    db_keyword = topic.get('keyword', '')
    db_category = topic.get('category', '')
    
    # NewsAPI å…è®¸ä½¿ç”¨ 'q' (å…³é”®è¯) å’Œ 'category' (åˆ†ç±»)
    # æˆ‘ä»¬å°†å®ƒä»¬ç»„åˆä½¿ç”¨ï¼Œå¹¶ç”¨ 'NOT æ”¿æ²»' æ¥è§„é¿é£é™©
    query = db_keyword
    
    # è®¡ç®—ä¸€å¤©å‰çš„æ—¶é—´ï¼Œåªçœ‹æœ€æ–°çš„
    yesterday = (datetime.now() - timedelta(days=1)).isoformat()
    
    params = {
        "q": query,
        "lang": "en",
        "max": ARTICLES_PER_TOPIC,
        "sortby": "publishedAt",
        "apikey": api_key,   # âœ… å®˜æ–¹æ¨èå‘½å
    }

    try:
        with httpx.Client(timeout=10.0) as client:
            response = client.get(NEWS_API_BASE_URL, params=params)
            response.raise_for_status() # å¦‚æœ API è¿”å› 4xx æˆ– 5xxï¼Œå°†å¼•å‘å¼‚å¸¸
            
            api_data = response.json()
            articles = api_data.get("articles", [])
            tqdm.write(f"  > API è¿”å›: ä¸»é¢˜ '{db_keyword}' æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ã€‚")
            return articles
            
    except httpx.HTTPStatusError as e:
        tqdm.write(f"ğŸ”´ é”™è¯¯: NewsAPI è¯·æ±‚å¤±è´¥ (HTTP {e.response.status_code})ï¼Œä¸»é¢˜: {db_keyword}")
    except httpx.RequestError as e:
        tqdm.write(f"ğŸ”´ é”™è¯¯: ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        tqdm.write(f"ğŸ”´ é”™è¯¯: API æ•°æ®è§£æå¤±è´¥: {e}")
    
    return []

def save_articles_to_db(articles: List[Dict[str, Any]], topic_id: int):
    """
    å°†ä» API è·å–çš„æ–‡ç« åˆ—è¡¨å­˜å…¥æ•°æ®åº“ã€‚
    [å¯¹åº” schema.sql è¡¨ 2]
    """
    if not articles:
        return 0
        
    db = get_db_client()
    new_articles_to_insert = []
    
    for article in articles:
        # æ ¼å¼åŒ–æ•°æ®ä»¥åŒ¹é…æˆ‘ä»¬çš„ 'raw_articles' è¡¨
        new_articles_to_insert.append({
            "topic_id": topic_id,
            "url": article.get("url"),
            "title": article.get("title"),
            "snippet": article.get("description") or article.get("content"),
            "source_name": article.get("source", {}).get("name"),
            "publication_date": article.get("publishedAt"),
            # crawl_date ä¼šè‡ªåŠ¨ç”±æ•°æ®åº“çš„ 'DEFAULT now()' å¡«å……
        })

    try:
        # **ã€æ ¸å¿ƒæˆæœ¬æ§åˆ¶ã€‘**
        # on_conflict="url" å‘Šè¯‰æ•°æ®åº“ï¼šå¦‚æœ 'url' å­—æ®µå·²å­˜åœ¨ï¼Œå°±å¿½ç•¥è¿™è¡Œæ•°æ®ã€‚
        # 'ignore_duplicates=True' æ˜¯ Supabase-Python åº“çš„å†™æ³•ã€‚
        # è¿™ç¡®ä¿æˆ‘ä»¬æ°¸è¿œä¸ä¼šé‡å¤æ’å…¥åŒä¸€ç¯‡æ–‡ç« ã€‚
        response = db.table("raw_articles").insert(
            new_articles_to_insert, 
            on_conflict="url", 
            ignore_duplicates=True
        ).execute()
        
        # response.data åŒ…å«äº† "æ–°" æ’å…¥çš„æ•°æ®æ¡ç›®
        inserted_count = len(response.data)
        return inserted_count
        
    except Exception as e:
        tqdm.write(f"ğŸ”´ é”™è¯¯: æ’å…¥æ–‡ç« åˆ° 'raw_articles' è¡¨å¤±è´¥: {e}")
        return 0

def main():
    """
    çˆ¬è™«ä¸»å‡½æ•°
    """
    print("--- çˆ¬è™«è„šæœ¬ (crawler.py) å¯åŠ¨ ---")
    
    news_api_key = os.environ.get("NEWS_API_KEY")
    if not news_api_key:
        print("ğŸ”´ é”™è¯¯: NEWS_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼çˆ¬è™«æ— æ³•è¿è¡Œã€‚")
        return

    # 1. è·å–è¦è¿½è¸ªçš„ä¸»é¢˜
    topics = fetch_topics_from_db()
    if not topics:
        print("â¹ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰æ¿€æ´»çš„ä¸»é¢˜ã€‚çˆ¬è™«é€€å‡ºã€‚")
        return
        
    total_new_articles = 0
    
    # 2. éå†æ¯ä¸ªä¸»é¢˜å¹¶çˆ¬å–
    print("  (Crawler Step 2/3) æ­£åœ¨ä» NewsAPI è·å–æ–‡ç« ...")
    with tqdm(total=len(topics), desc="å¤„ç†ä¸»é¢˜") as pbar:
        for topic in topics:
            pbar.set_description(f"å¤„ç†ä¸­: {topic['keyword']}")
            
            # 3. ä» API è·å–æ–‡ç« 
            articles = fetch_articles_from_api(topic, news_api_key)
            
            if articles:
                # 4. ä¿å­˜åˆ°æ•°æ®åº“ (æ­¤æ­¥éª¤ä¼šè‡ªåŠ¨å»é‡)
                new_count = save_articles_to_db(articles, topic['topic_id'])
                total_new_articles += new_count
                tqdm.write(f"  > å­˜å‚¨: ä¸»é¢˜ '{topic['keyword']}' æ–°å¢ {new_count} ç¯‡æ–‡ç« åˆ°æ•°æ®åº“ã€‚")
            
            pbar.update(1)

    print("  (Crawler Step 3/3) çˆ¬å–å®Œæˆã€‚")
    print(f"--- çˆ¬è™«è„šæœ¬ (crawler.py) ç»“æŸ ---")
    print(f"ğŸŸ¢ æ€»ç»“ï¼šæ€»å…±å‘ç° {total_new_articles} ç¯‡æ–°æ–‡ç« å¹¶å­˜å…¥æ•°æ®åº“ã€‚")


if __name__ == "__main__":
    main()