import os
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
from tqdm import tqdm
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import ValidationError

# å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„æ¨¡å—
from .db import get_db_client
from .l1_structure import L1AnalysisStructure

# -----------------------------------------------------------------
# å¸¸é‡å®šä¹‰ (Constants)
# -----------------------------------------------------------------
# ä»ç¯å¢ƒå˜é‡ä¸­è·å– AI é…ç½®ï¼Œä½¿ç”¨åŸä»“åº“çš„å˜é‡å
MODEL_NAME = os.environ.get("MODEL_NAME", "deepseek-chat")
LANGUAGE = os.environ.get("LANGUAGE", "Chinese")
# å¹¶è¡Œå¤„ç†çš„å·¥ä½œçº¿ç¨‹æ•°ï¼Œå°±åƒåŸä»“åº“çš„ 'max_workers'
MAX_WORKERS = 2 

def load_prompt() -> str:
    """ä»æ–‡ä»¶åŠ è½½ L1 æç¤ºè¯"""
    prompt_path = os.path.join(os.path.dirname(__file__), 'prompts', 'l1_analysis.txt')
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()

def get_unanalyzed_articles() -> List[Dict[str, Any]]:
    """
    ä»æ•°æ®åº“è·å–æ‰€æœ‰â€œæœªè¢«åˆ†æè¿‡â€çš„æ–‡ç«  (L0)ã€‚
    è¿™æ˜¯é€šè¿‡ 'LEFT JOIN' å®ç°çš„ï¼šæˆ‘ä»¬æŸ¥æ‰¾æ‰€æœ‰åœ¨ 'raw_articles' ä¸­
    ä½†â€œä¸åœ¨â€ 'l1_analysis_sentiment' è¡¨ä¸­çš„æ–‡ç« ã€‚
    """
    print("  (Analysis Step 1/3) æ­£åœ¨ä»æ•°æ®åº“è·å–â€œæœªåˆ†æâ€çš„æ–‡ç« ...")
    db = get_db_client()
    try:
        # ä½¿ç”¨ PostgREST çš„ RPC (è¿œç¨‹è¿‡ç¨‹è°ƒç”¨) æˆ–è§†å›¾ (View) æ˜¯æœ€é«˜æ•ˆçš„
        # ä½†ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç­‰æ•ˆçš„ 'left_join' æŸ¥è¯¢
        # (æ³¨æ„: Supabase-py v2+ å¯èƒ½éœ€è¦è°ƒæ•´æ­¤æŸ¥è¯¢è¯­æ³•)
        #
        # SQL ç­‰ä»·äº:
        # SELECT a.article_id, a.title, a.snippet, t.keyword 
        # FROM raw_articles a
        # JOIN tracked_topics t ON a.topic_id = t.topic_id
        # LEFT JOIN l1_analysis_sentiment s ON a.article_id = s.article_id
        # WHERE s.analysis_id IS NULL;

        response = db.table("raw_articles").select(
            "article_id, title, snippet, tracked_topics(keyword), l1_analysis_sentiment(analysis_id)"
        ).is_("l1_analysis_sentiment.analysis_id", None).execute()
        
        articles = response.data
        tqdm.write(f"  > æˆåŠŸè·å– {len(articles)} ç¯‡æ–°æ–‡ç« å¾…åˆ†æã€‚")
        return articles
    except Exception as e:
        tqdm.write(f"ğŸ”´ é”™è¯¯: æ— æ³•è·å–æœªåˆ†æçš„æ–‡ç« : {e}")
        return []

def process_single_article(article: Dict[str, Any], chain) -> Dict[str, Any] | None:
    """
    ä½¿ç”¨ AI chain å¤„ç†å•ç¯‡æ–‡ç« ï¼Œå¹¶è¿”å›ç»“æ„åŒ–æ•°æ®ã€‚
    """
    try:
        # å‡†å¤‡ AI æ¨¡å‹çš„è¾“å…¥
        # (æ³¨æ„ï¼š'tracked_topics' æ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæˆ‘ä»¬éœ€è¦æå–ç¬¬ä¸€ä¸ª)
        topic_keyword = "general"
        if article.get('tracked_topics'):
            topic_keyword = article['tracked_topics']['keyword']
            
        ai_input = {
            "language": LANGUAGE,
            "topic_keyword": topic_keyword,
            "article_title": article['title'],
            "article_snippet": article['snippet']
        }
        
        # è°ƒç”¨ AI (è¿™æ­¥æœ€è€—æ—¶)
        response: L1AnalysisStructure = chain.invoke(ai_input)
        
        # å°†ç»“æœä¸æ–‡ç«  ID ç»‘å®šï¼Œä»¥ä¾¿ç¨åå­˜å…¥æ•°æ®åº“
        return {"article_id": article['article_id'], "analysis": response}
        
    except ValidationError as e:
        tqdm.write(f"ğŸŸ¡ AI è¾“å‡ºè§£æå¤±è´¥ (ID: {article['article_id']}): {e}")
    except Exception as e:
        tqdm.write(f"ğŸ”´ AI è°ƒç”¨å¤±è´¥ (ID: {article['article_id']}): {e}")
    
    return None

def save_analysis_to_db(result: Dict[str, Any]):
    """
    å°†å•ç¯‡ AI åˆ†æç»“æœï¼ˆL1ï¼‰å­˜å…¥æ•°æ®åº“çš„ä¸‰ä¸ªè¡¨ä¸­ã€‚
    [å¯¹åº” schema.sql è¡¨ 3, 4, 5]
    """
    db = get_db_client()
    article_id = result['article_id']
    analysis = result['analysis']

    try:
        # 1. å†™å…¥ 'l1_analysis_sentiment' è¡¨ (è¡¨ 3)
        db.table("l1_analysis_sentiment").insert({
            "article_id": article_id,
            "ai_summary": analysis.ai_summary,
            "sentiment_score": analysis.sentiment_score,
            "sentiment_label": analysis.sentiment_label
        }).execute()
        
        # 2. & 3. å†™å…¥ 'l1_analysis_entities' (è¡¨ 4) å’Œ 'article_entity_map' (è¡¨ 5)
        if analysis.entities:
            entity_ids_to_map = []
            
            # å‡†å¤‡å®ä½“æ•°æ®
            entities_to_upsert = [
                {"entity_name": e.name, "entity_type": e.type} 
                for e in analysis.entities
            ]
            
            # 'upsert' ä¼šæ’å…¥æ–°å®ä½“ï¼Œæˆ–åœ¨ 'entity_name' å†²çªæ—¶æ›´æ–°ç°æœ‰å®ä½“
            # è¿™èƒ½ç¡®ä¿ 'NVIDIA' åœ¨ 'l1_analysis_entities' ä¸­åªå­˜åœ¨ä¸€æ¬¡ (è§„èŒƒåŒ–)
            entity_response = db.table("l1_analysis_entities").upsert(
                entities_to_upsert, 
                on_conflict="entity_name"
            ).execute()
            
            entity_ids = [e['entity_id'] for e in entity_response.data]
            
            # å‡†å¤‡è¿æ¥è¡¨æ•°æ® (å¤šå¯¹å¤šå…³ç³»)
            map_data_to_insert = [
                {"article_id": article_id, "entity_id": eid}
                for eid in entity_ids
            ]

            # æ’å…¥è¿æ¥è¡¨ï¼Œ'ignore_duplicates=True' é˜²æ­¢é‡å¤
            db.table("article_entity_map").insert(
                map_data_to_insert,
                on_conflict="article_id, entity_id",
                ignore_duplicates=True
            ).execute()

        return True # è¡¨ç¤ºæˆåŠŸ
        
    except Exception as e:
        tqdm.write(f"ğŸ”´ æ•°æ®åº“å†™å…¥å¤±è´¥ (ID: {article_id}): {e}")
        # (å¯é€‰) åœ¨è¿™é‡Œæ·»åŠ é€»è¾‘ï¼Œåˆ é™¤å·²æ’å…¥çš„ l1_analysis_sentimentï¼Œä»¥å®ç°äº‹åŠ¡å›æ»š
        db.table("l1_analysis_sentiment").delete().eq("article_id", article_id).execute()
        return False # è¡¨ç¤ºå¤±è´¥

def main():
    """
    L1 åˆ†æè„šæœ¬ä¸»å‡½æ•°
    """
    print("--- L1 åˆ†æè„šæœ¬ (analysis.py) å¯åŠ¨ ---")
    
    # 1. åˆå§‹åŒ– AI
    try:
        l1_prompt_template = load_prompt()
        prompt = ChatPromptTemplate.from_template(l1_prompt_template)
        
        # ä½¿ç”¨ Pydantic æ¨¡å‹å¼ºåˆ¶ AI è¾“å‡º JSON
        llm = ChatOpenAI(model=MODEL_NAME).with_structured_output(L1AnalysisStructure)
        
        chain = prompt | llm
        print(f"  > AI æ¨¡å‹ ({MODEL_NAME}) å’Œæç¤ºè¯å·²åŠ è½½ã€‚")
    except Exception as e:
        print(f"ğŸ”´ è‡´å‘½é”™è¯¯: æ— æ³•åˆå§‹åŒ– AI: {e}")
        return

    # 2. è·å–å¾…å¤„ç†çš„æ–‡ç« 
    articles_to_process = get_unanalyzed_articles()
    if not articles_to_process:
        print("â¹ï¸ æ²¡æœ‰æ–°æ–‡ç« éœ€è¦åˆ†æã€‚è„šæœ¬é€€å‡ºã€‚")
        return
        
    print(f"  (Analysis Step 2/3) å¼€å§‹ä½¿ç”¨ {MAX_WORKERS} ä¸ªå¹¶è¡Œçº¿ç¨‹å¤„ç† {len(articles_to_process)} ç¯‡æ–‡ç« ...")
    
    successful_analyses = 0
    
    # 3. å¹¶è¡Œè°ƒç”¨ AI (å¤ç”¨åŸä»“åº“çš„å¤šçº¿ç¨‹é€»è¾‘)
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ AI åˆ†æä»»åŠ¡
        future_to_article = {
            executor.submit(process_single_article, article, chain): article
            for article in articles_to_process
        }
        
        # æ”¶é›† AI åˆ†æç»“æœ (å¸¦è¿›åº¦æ¡)
        ai_results = []
        for future in tqdm(as_completed(future_to_article), total=len(articles_to_process), desc="AI åˆ†æ (L1)"):
            result = future.result()
            if result:
                ai_results.append(result)

    print(f"  > AI åˆ†æå®Œæˆã€‚æˆåŠŸ {len(ai_results)} ç¯‡ï¼Œå¤±è´¥ {len(articles_to_process) - len(ai_results)} ç¯‡ã€‚")

    # 4. å°† AI ç»“æœå­˜å…¥æ•°æ®åº“
    if ai_results:
        print(f"  (Analysis Step 3/3) æ­£åœ¨å°† {len(ai_results)} ç¯‡åˆ†æç»“æœå­˜å…¥æ•°æ®åº“...")
        with tqdm(total=len(ai_results), desc="æ•°æ®åº“å†™å…¥ (L1)") as pbar:
            for result in ai_results:
                if save_analysis_to_db(result):
                    successful_analyses += 1
                pbar.update(1)

    print("--- L1 åˆ†æè„šæœ¬ (analysis.py) ç»“æŸ ---")
    print(f"ğŸŸ¢ æ€»ç»“ï¼šæ€»å…± {successful_analyses} ç¯‡æ–°æ–‡ç« çš„ L1 åˆ†æå·²æˆåŠŸå­˜å…¥æ•°æ®åº“ã€‚")

if __name__ == "__main__":
    main()