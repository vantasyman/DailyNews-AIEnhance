import os
import sys
import json
from datetime import datetime, timedelta
from tqdm import tqdm
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import ValidationError
from collections import defaultdict
from typing import List, Dict, Any # â¬…ï¸ å¯¼å…¥ Any

# å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„æ¨¡å—
from .db import get_db_client
from .l2_structure import L2ReportStructure

# -----------------------------------------------------------------
# å¸¸é‡å®šä¹‰ (Constants)
# -----------------------------------------------------------------
MODEL_NAME = os.environ.get("MODEL_NAME", "deepseek-chat")
LANGUAGE = os.environ.get("LANGUAGE", "Chinese")
# ã€æ–°ã€‘å®šä¹‰ L2 æŠ¥å‘Šè¦æ˜¾ç¤ºçš„çƒ­é—¨å®ä½“æ•°é‡
TOP_N_ENTITIES = 5 

def load_prompt() -> str:
    """ä»æ–‡ä»¶åŠ è½½ L2 æç¤ºè¯"""
    prompt_path = os.path.join(os.path.dirname(__file__), 'prompts', 'l2_report.txt')
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()

def get_l1_data_for_report() -> Dict[str, List[Dict]]:
    """
    ã€ä¿®æ”¹ã€‘è·å–è¿‡å»24hçš„ L1 æ‘˜è¦æ•°æ® (ä¸ä¹‹å‰ç›¸åŒ)
    """
    print("  (Report Step 1/4) æ­£åœ¨ä»æ•°æ®åº“è·å–è¿‡å» 24h çš„ L1 æ‘˜è¦æ•°æ®...")
    db = get_db_client()
    
    time_threshold = (datetime.now() - timedelta(days=1)).isoformat()
    
    try:
        # æ­¤æŸ¥è¯¢ä¿æŒä¸å˜
        response = db.table("l1_analysis_sentiment").select(
            """
            analyzed_at,
            raw_articles (
                title,
                tracked_topics ( category )
            ),
            ai_summary,
            sentiment_score
            """
        ).gte("analyzed_at", time_threshold).execute()

        data = response.data
        
        grouped_data = defaultdict(list)
        for item in data:
            if not item.get('raw_articles') or not item['raw_articles'].get('tracked_topics'):
                continue
                
            category = item['raw_articles']['tracked_topics']['category']
            grouped_data[category].append({
                "title": item['raw_articles']['title'],
                "summary": item['ai_summary'],
                "sentiment_score": item['sentiment_score']
            })
            
        tqdm.write(f"  > æˆåŠŸè·å– {len(data)} æ¡ L1 æ‘˜è¦ï¼Œåˆ†å± {len(grouped_data)} ä¸ªåˆ†ç±»ã€‚")
        return grouped_data

    except Exception as e:
        tqdm.write(f"ğŸ”´ é”™è¯¯: æ— æ³•è·å– L1 æ‘˜è¦æ•°æ®: {e}")
        return {}

def get_grouped_trending_entities() -> Dict[str, List[Dict]]:
    """
    ã€æ–°å¢ã€‘ä» 'daily_trending_entities' è§†å›¾ä¸­è·å–å·²èšåˆçš„çƒ­é—¨å®ä½“æ•°æ®ã€‚
    """
    print("  (Report Step 2/4) æ­£åœ¨ä»æ•°æ®åº“è§†å›¾è·å–çƒ­é—¨å®ä½“æ•°æ®...")
    db = get_db_client()
    try:
        # ç›´æ¥æŸ¥è¯¢æˆ‘ä»¬åˆ›å»ºçš„è§†å›¾
        response = db.table("daily_trending_entities").select("*").execute()
        
        grouped_entities = defaultdict(list)
        for entity in response.data:
            # è§†å›¾è¿”å›çš„æ•°æ®å­—æ®µå·²å®Œç¾åŒ¹é… l2_structure.py ä¸­çš„ TrendingTopic æ¨¡å‹
            #
            grouped_entities[entity['category']].append(entity)
            
        tqdm.write(f"  > æˆåŠŸè·å– {len(response.data)} æ¡çƒ­é—¨å®ä½“æ•°æ®ã€‚")
        return grouped_entities
        
    except Exception as e:
        # å¦‚æœè§†å›¾ä¸å­˜åœ¨ (e.g., SQL æœªè¿è¡Œ)ï¼Œè¿™é‡Œä¼šæŠ¥é”™
        tqdm.write(f"ğŸ”´ é”™è¯¯: æ— æ³•ä» 'daily_trending_entities' è§†å›¾è·å–æ•°æ®: {e}")
        tqdm.write("   è¯·ç¡®ä¿ä½ å·²åœ¨æ•°æ®åº“ä¸­è¿è¡Œäº† schema.sql ä¸­çš„ CREATE VIEW è¯­å¥ã€‚")
        return {}

def generate_l2_report(
    category: str, 
    l1_article_data: List[Dict], 
    l1_entity_data: List[Dict], 
    chain
) -> L2ReportStructure | None:
    """
    ã€ä¿®æ”¹ã€‘ä¸ºå•ä¸ªåˆ†ç±»è°ƒç”¨ AIï¼ŒåŒæ—¶æ³¨å…¥â€œæ‘˜è¦â€å’Œâ€œå®ä½“â€
    """
    try:
        # 1. å‡†å¤‡ L1 æ‘˜è¦ JSON
        l1_data_json = json.dumps(l1_article_data, ensure_ascii=False, indent=2)
        
        # 2. ã€æ–°ã€‘å‡†å¤‡ L1 å®ä½“ JSON (åªå– Top N)
        top_entities = l1_entity_data[:TOP_N_ENTITIES]
        entity_data_json = json.dumps(top_entities, ensure_ascii=False, indent=2)

        # 3. å‡†å¤‡ AI è¾“å…¥
        ai_input = {
            "language": LANGUAGE,
            "category": category,
            "l1_data_json": l1_data_json,
            "entity_data_json": entity_data_json # â¬…ï¸ ã€æ–°ã€‘æ³¨å…¥å®ä½“æ•°æ®
        }
        
        response: L2ReportStructure = chain.invoke(ai_input)
        
        # 4. ã€æ–°ã€‘å°†æˆ‘ä»¬é¢„å…ˆè®¡ç®—çš„å®ä½“æ•°æ®â€œè¦†ç›–â€å› AI å“åº”
        #    æˆ‘ä»¬ä¿¡ä»»è‡ªå·±çš„èšåˆæ•°æ®ï¼ŒAI çš„èŒè´£æ˜¯åŸºäºè¿™äº›æ•°æ®å†™æ‘˜è¦ã€‚
        #    (è¿™ä¹Ÿé˜²æ­¢äº† AI åœ¨æ­¤æ­¥éª¤ä¸­äº§ç”Ÿå¹»è§‰æˆ–æ ¼å¼é”™è¯¯)
        
        # ç¡®ä¿ AI è¿”å›çš„ç»“æ„æ˜¯æˆ‘ä»¬æƒ³è¦çš„
        final_report = response.model_copy() # å¤åˆ¶ AI çš„è¾“å‡º
        
        # å°† Pydantic æ¨¡å‹åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œä»¥ä¾¿å­˜å…¥ JSONB
        #
        final_report.trending_topics = [
            {"topic": e['topic'], "count": e['count'], "average_sentiment": e['average_sentiment']}
            for e in top_entities
        ]

        return final_report
        
    except ValidationError as e:
        tqdm.write(f"ğŸŸ¡ AI è¾“å‡ºè§£æå¤±è´¥ (åˆ†ç±»: {category}): {e}")
    except Exception as e:
        tqdm.write(f"ğŸ”´ AI è°ƒç”¨å¤±è´¥ (åˆ†ç±»: {category}): {e}")
    
    return None

def save_l2_report_to_db(category: str, report: L2ReportStructure):
    """
    å°† L2 æŠ¥å‘Šå­˜å…¥æ•°æ®åº“ 'daily_reports' (è¡¨ 6)ã€‚
    (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œä½†è¯·æ³¨æ„æˆ‘ä»¬ä¿®å¤äº† schema.sql ä¸­çš„å­—æ®µå)
    """
    db = get_db_client()
    today = datetime.now().date()
    
    try:
        report_data = {
            "report_date": str(today),
            "category": category,
            "report_summary": report.report_summary,
            "overall_sentiment_score": report.overall_sentiment_score,
            # 'trending_topics' æ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨ (æˆ‘ä»¬å·²åœ¨ generate_l2_report ä¸­å¤„ç†)
            "trending_topics": report.trending_topics 
        }
        
        # 'upsert' ä¼šåœ¨ (report_date, category) å†²çªæ—¶â€œæ›´æ–°â€æŠ¥å‘Š
        db.table("daily_reports").upsert(
            report_data, 
            on_conflict="report_date, category"
        ).execute()
        return True
        
    except Exception as e:
        tqdm.write(f"ğŸ”´ æ•°æ®åº“å†™å…¥ L2 æŠ¥å‘Šå¤±è´¥ (åˆ†ç±»: {category}): {e}")
        return False

def main():
    """
    L2 æŠ¥å‘Šè„šæœ¬ä¸»å‡½æ•°
    """
    print("--- L2 æŠ¥å‘Šè„šæœ¬ (report.py) å¯åŠ¨ ---")
    
    # 1. åˆå§‹åŒ– AI (ä¸å˜)
    try:
        # ã€æ–°ã€‘å¯¼å…¥ Pydantic è§£æå™¨
        from langchain_core.output_parsers import PydanticOutputParser
        
        # 1. åŠ è½½åŸå§‹æç¤ºè¯å­—ç¬¦ä¸²
        l2_prompt_template_str = load_prompt()
        
        # 2. è®¾ç½®æˆ‘ä»¬çš„è§£æå™¨
        parser = PydanticOutputParser(pydantic_object=L2ReportStructure)
        
        # 3. ä»è§£æå™¨è·å– JSON æ ¼å¼åŒ–æŒ‡ä»¤
        format_instructions = parser.get_format_instructions()
        
        # 4. ã€å…³é”®ã€‘å°†æ ¼å¼åŒ–æŒ‡ä»¤é™„åŠ åˆ°åŸå§‹æç¤ºè¯çš„æœ«å°¾
        l2_prompt_template_str += "\n\n{format_instructions}\n"
        
        # 5. åˆ›å»ºæ–°çš„ã€åŒ…å«æ ¼å¼åŒ–æŒ‡ä»¤çš„ PromptTemplate
        prompt = ChatPromptTemplate.from_template(
            l2_prompt_template_str,
            partial_variables={"format_instructions": format_instructions}
        )
        
        # 6. ã€ä¿®å¤ã€‘åˆå§‹åŒ– LLMï¼Œä½†*ä¸*ä½¿ç”¨ .with_structured_output()
        llm = ChatOpenAI(model=MODEL_NAME)
        
        # 7. åˆ›å»ºæ–°çš„ chain
        chain = prompt | llm | parser

        print(f"  > L2 AI æ¨¡å‹ ({MODEL_NAME}) å’Œæç¤ºè¯å·²åŠ è½½ (ä½¿ç”¨ PydanticParser)ã€‚")
    except Exception as e:
        print(f"ğŸ”´ è‡´å‘½é”™è¯¯: æ— æ³•åˆå§‹åŒ– L2 AI: {e}")
        return

    # 2. ã€ä¿®æ”¹ã€‘è·å– L1 æ‘˜è¦ å’Œ L1 å®ä½“
    grouped_l1_data = get_l1_data_for_report()
    grouped_entity_data = get_grouped_trending_entities() # â¬…ï¸ ã€æ–°ã€‘
    
    if not grouped_l1_data:
        print("â¹ï¸ è¿‡å» 24 å°æ—¶æ²¡æœ‰æ–°çš„ L1 åˆ†ææ•°æ®ã€‚è„šæœ¬é€€å‡ºã€‚")
        return
        
    print(f"  (Report Step 3/4) å¼€å§‹ä¸º {len(grouped_l1_data)} ä¸ªåˆ†ç±»ç”Ÿæˆ L2 æŠ¥å‘Š...")
    
    successful_reports = 0
    
    # 3. éå†æ¯ä¸ªåˆ†ç±»
    with tqdm(total=len(grouped_l1_data), desc="ç”Ÿæˆ L2 æŠ¥å‘Š") as pbar:
        for category, l1_data in grouped_l1_data.items():
            pbar.set_description(f"L2 æŠ¥å‘Š: {category}")
            
            # ã€æ–°ã€‘è·å–è¯¥åˆ†ç±»å¯¹åº”çš„å®ä½“æ•°æ®
            entities_for_category = grouped_entity_data.get(category, [])
            
            # 4. è°ƒç”¨ AI ç”ŸæˆæŠ¥å‘Š (ä¼ å…¥ä¸¤ç§æ•°æ®)
            report = generate_l2_report(category, l1_data, entities_for_category, chain)
            
            # 5. å­˜å…¥æ•°æ®åº“
            if report:
                if save_l2_report_to_db(category, report):
                    successful_reports += 1
            
            pbar.update(1)

    print(f"  (Report Step 4/4) L2 æŠ¥å‘Šå¤„ç†å®Œæˆã€‚")
    print("--- L2 æŠ¥å‘Šè„šæœ¬ (report.py) ç»“æŸ ---")
    print(f"ğŸŸ¢ æ€»ç»“ï¼šæ€»å…± {successful_reports} ä»½ L2 æ¯æ—¥æŠ¥å‘Šå·²æˆåŠŸå­˜å…¥æ•°æ®åº“ã€‚")

if __name__ == "__main__":
    main()